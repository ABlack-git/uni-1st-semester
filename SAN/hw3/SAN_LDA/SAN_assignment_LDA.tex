\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Linear Discriminanat Analysis},
            pdfauthor={Andrii Zakharchenko},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
% grffile has become a legacy package: https://ctan.org/pkg/grffile
\IfFileExists{grffile.sty}{%
\usepackage{grffile}
}{}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\providecommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{Linear Discriminanat Analysis}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{Andrii Zakharchenko}
    \preauthor{\centering\large\emph}
  \postauthor{\par}
      \predate{\centering\large\emph}
  \postdate{\par}
    \date{November 4, 2019}


\begin{document}
\maketitle

\hypertarget{introduction}{%
\paragraph{Introduction}\label{introduction}}

The aim of this assignment is to get familiar with Linear Discriminant
Analysis (LDA). LDA and Principal Component Analysis (PCA) are two
techniques for dimensionality reduction. PCA can be decribed as an
unsupervised algorithm that ignores data labels and aims to find
directions which maximalize the variance in a data. In comparison with
PCA, LDA is a supervised algorithm and aims to project a dataset onto a
lower dimensional space with good class separability. In other words,
LDA maximalizes the ratio of betweenclass variance and the within-class
variance in a given data.

The deadline of this assignment is November 25.

\hypertarget{input-data}{%
\paragraph{Input data}\label{input-data}}

In this tutorial, we will work with a dataset that classifies wines
(samples) into three classes using of 13 continuous attributes; for more
details see wine info.txt file. The dataset is located at wine.csv.

\hypertarget{linear-discriminant-analysis}{%
\paragraph{Linear Discriminant
Analysis}\label{linear-discriminant-analysis}}

As we mentioned above, LDA finds directions where classes are
well-separated, i.e.~LDA maximizes the ratio of between-class variance
and the within-class variance. Firstly, assume that \(C\) is a set of
classes and set \(D\), which represents a training dataset, is defined
as \(D = \{x_1, x_2, . . . , x_N \}\).

The between-classes scatter matrix SB is defined as:
\(S_b = \sum_c N_C(\mu_c -\overline{x})(\mu_c - \overline{x})^T\), where
\(\overline{x}\) is a vector represents the overall mean of the data, Âµ
represents the mean corresponding to each class, and \(N_C\) are sizes
of the respective classes.

The within-classes scatter matrix \(S_W\) is defined as:

\(S_W = \sum_c \sum_{x \in D_c}(x - \overline{\mu_c})(x - \overline{\mu_c})^T\)

Next, we will solve the generalized eigenvalue problem for the matrix
\(S_W^{-1}S_B\) to obtain the linear discriminants, i.e.

\((S_W^{-1}S_B)w = \lambda w\)

where \(w\) represents an eigenvector and \(\lambda\) represents an
eigenvalue. Finally, choose k eigenvectors with the largest eigenvalue
and transform the samples onto the new subspace.

\hypertarget{step-by-step}{%
\paragraph{Step by step}\label{step-by-step}}

\hypertarget{load-the-dataset}{%
\subparagraph{Load the dataset}\label{load-the-dataset}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# ADD YOUR CODE}
\end{Highlighting}
\end{Shaded}

\hypertarget{compute-the-within-scatter-matrix}{%
\subparagraph{Compute the within-scatter
matrix}\label{compute-the-within-scatter-matrix}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ComputeWithinScatter <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(data, n)}
\NormalTok{\{}
  \CommentTok{# ADD YOUR CODE}
\NormalTok{  withinMatrix <-}\StringTok{ }\DecValTok{0}
  \KeywordTok{return}\NormalTok{(withinMatrix)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\hypertarget{compute-the-between-scatter-matrix}{%
\subparagraph{Compute the between-scatter
matrix}\label{compute-the-between-scatter-matrix}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ComputeBetweenScatter <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(data, n, meanOverall)}
\NormalTok{\{}
  \CommentTok{# ADD YOUR CODE}
\NormalTok{  betweenMatrix <-}\StringTok{ }\DecValTok{0}
  \KeywordTok{return}\NormalTok{(betweenMatrix)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\hypertarget{solve-the-eigenproblem-and-return-eigen-vector}{%
\subparagraph{Solve the EigenProblem and return
eigen-vector}\label{solve-the-eigenproblem-and-return-eigen-vector}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SolveEigenProblem <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(withinMatrix, betweenMatrix, prior)}
\NormalTok{\{}
  \CommentTok{# ADD YOUR CODE}
\NormalTok{  eivectors <-}\StringTok{ }\DecValTok{0}
  \KeywordTok{return}\NormalTok{(eivectors)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\hypertarget{visualize-the-results}{%
\subparagraph{Visualize the results}\label{visualize-the-results}}

Project your data into lower-dimensional subspace, visualize this
projection, and compare with PCA (see Fig. 1). Also, try to use
scale/unscale version of \texttt{prcomp} function in R. Use the
following code while filling in the lines marked as \texttt{TODO}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ComputeCentroids <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(data, labels)\{}
\NormalTok{  yGroupedMean <-}\StringTok{ }\KeywordTok{aggregate}\NormalTok{(}\KeywordTok{as.data.frame}\NormalTok{(data), }\DataTypeTok{by =} \KeywordTok{list}\NormalTok{(labels), }\DataTypeTok{FUN =}\NormalTok{ mean)}
  \KeywordTok{rownames}\NormalTok{(yGroupedMean) <-}\StringTok{ }\NormalTok{yGroupedMean[,}\DecValTok{1}\NormalTok{]}
\NormalTok{  yGroupedMean <-}\StringTok{ }\NormalTok{yGroupedMean[,}\OperatorTok{-}\DecValTok{1}\NormalTok{]}
  \KeywordTok{return}\NormalTok{(yGroupedMean)}
\NormalTok{\}}

\NormalTok{Classify <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(newData, eigenVectors, labels, centroids)\{}
\NormalTok{  y <-}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(newData) }\OperatorTok{%*%}\StringTok{ }\NormalTok{eigenVectors[,}\DecValTok{1}\OperatorTok{:}\NormalTok{(}\KeywordTok{length}\NormalTok{(}\KeywordTok{levels}\NormalTok{(labels))}\OperatorTok{-}\DecValTok{1}\NormalTok{)]}
\NormalTok{  prior <-}\StringTok{ }\KeywordTok{table}\NormalTok{(labels)}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(}\KeywordTok{table}\NormalTok{(labels))}
  
\NormalTok{  classification <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\DataTypeTok{nrow =} \KeywordTok{nrow}\NormalTok{(newData), }\DataTypeTok{ncol =} \KeywordTok{length}\NormalTok{(}\KeywordTok{levels}\NormalTok{(labels)))}
  \KeywordTok{colnames}\NormalTok{(classification) <-}\StringTok{ }\KeywordTok{levels}\NormalTok{(labels)}
  \ControlFlowTok{for}\NormalTok{(c }\ControlFlowTok{in} \KeywordTok{levels}\NormalTok{(labels))}
\NormalTok{  \{}
\NormalTok{    classification[,c] <-}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(}\FloatTok{0.5}\OperatorTok{*}\KeywordTok{rowSums}\NormalTok{((y }\OperatorTok{-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\KeywordTok{as.matrix}\NormalTok{(centroids[c,]),}
                                                                \KeywordTok{nrow}\NormalTok{(newData)), }\DataTypeTok{nrow =} \KeywordTok{nrow}\NormalTok{(newData),}
                                                            \DataTypeTok{byrow =} \OtherTok{TRUE}\NormalTok{) )}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
                                    \OperatorTok{-}\StringTok{ }\KeywordTok{log}\NormalTok{(prior[c]))}
\NormalTok{  \}}
  \KeywordTok{return}\NormalTok{(}\KeywordTok{levels}\NormalTok{(labels)[}\KeywordTok{apply}\NormalTok{(classification, }\DataTypeTok{MARGIN =} \DecValTok{1}\NormalTok{, which.min)])}
\NormalTok{\}}

\NormalTok{CrossvalidationLDA <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(mydata, labels, }\DataTypeTok{kfolds =} \DecValTok{10}\NormalTok{)\{}
  \KeywordTok{set.seed}\NormalTok{(}\DecValTok{17}\NormalTok{)}
  \CommentTok{#randomly shuffle the data}
\NormalTok{  random <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(mydata))}
\NormalTok{  data <-mydata[random,]}
\NormalTok{  labels <-}\StringTok{ }\NormalTok{labels[random]}
  \CommentTok{#Create 10 equally size folds}
\NormalTok{  folds <-}\StringTok{ }\KeywordTok{cut}\NormalTok{(}\KeywordTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{,}\KeywordTok{nrow}\NormalTok{(data)),}\DataTypeTok{breaks=}\NormalTok{kfolds,}\DataTypeTok{labels=}\OtherTok{FALSE}\NormalTok{)}
\NormalTok{  acc <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DataTypeTok{times =}\NormalTok{ kfolds)}
  \CommentTok{#10 fold cross validation}
  \ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{kfolds)\{}
    \CommentTok{#Segment your data by fold using the which() function }
\NormalTok{    testIndexes <-}\StringTok{ }\KeywordTok{which}\NormalTok{(folds}\OperatorTok{==}\NormalTok{i,}\DataTypeTok{arr.ind=}\OtherTok{TRUE}\NormalTok{)}
\NormalTok{    testData <-}\StringTok{ }\NormalTok{data[testIndexes, ]}
\NormalTok{    trainData <-}\StringTok{ }\NormalTok{data[}\OperatorTok{-}\NormalTok{testIndexes, ]}
\NormalTok{    testLabels <-}\StringTok{ }\NormalTok{labels[testIndexes]}
\NormalTok{    trainLabels <-}\StringTok{ }\NormalTok{labels[}\OperatorTok{-}\NormalTok{testIndexes]}
    
\NormalTok{    eigenLDA <-}\StringTok{ }\KeywordTok{LDA}\NormalTok{(trainData, trainLabels)}
\NormalTok{    centroids <-}\StringTok{ }\KeywordTok{ComputeCentroids}\NormalTok{(}\KeywordTok{as.matrix}\NormalTok{(trainData) }\OperatorTok{%*%}\StringTok{ }\NormalTok{eigenLDA[,}\DecValTok{1}\OperatorTok{:}\NormalTok{(}\KeywordTok{length}\NormalTok{(}\KeywordTok{levels}\NormalTok{(trainLabels))}\OperatorTok{-}\DecValTok{1}\NormalTok{)],}
                                  \DataTypeTok{labels =}\NormalTok{ trainLabels)}
\NormalTok{    pre <-}\StringTok{ }\KeywordTok{Classify}\NormalTok{(}\DataTypeTok{newData =}\NormalTok{ testData, }\DataTypeTok{labels =}\NormalTok{ trainLabels, }\DataTypeTok{eigenVectors =}\NormalTok{ eigenLDA,}
                    \DataTypeTok{centroids =}\NormalTok{ centroids)}
\NormalTok{    acc[i] <-}\StringTok{ }\KeywordTok{sum}\NormalTok{(pre }\OperatorTok{==}\StringTok{ }\NormalTok{testLabels)}\OperatorTok{/}\KeywordTok{length}\NormalTok{(testLabels)}
\NormalTok{  \}}
  \KeywordTok{return}\NormalTok{(}\KeywordTok{mean}\NormalTok{(acc))}
\NormalTok{\}}

\NormalTok{LDA <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(mydata, labels)\{}

  \CommentTok{#number of classes}
\NormalTok{  n <-}\KeywordTok{length}\NormalTok{(}\KeywordTok{levels}\NormalTok{(labels))}

  \CommentTok{# 1) split the data w.r.t. given factors}
\NormalTok{  splittedData <-}\StringTok{ }\KeywordTok{split}\NormalTok{(mydata, labels)}
  
  \CommentTok{# 2) scatter matrices}
  \CommentTok{#############  within-class scatter matrix Sw ##################}
\NormalTok{  withinScatterMatrix <-}\StringTok{ }\KeywordTok{ComputeWithinScatter}\NormalTok{(?) }\CommentTok{#TODO}
  
  \CommentTok{#############  between-class scatter matrix Sb ##################}
\NormalTok{  betweenScatterMatrix <-}\StringTok{ }\KeywordTok{ComputeBetweenScatter}\NormalTok{(?) }\CommentTok{#TODO}
  
  \CommentTok{# 3)  eigen problem}
  \CommentTok{############ solve Eigen problem ################################}
\NormalTok{  ei <-}\StringTok{ }\KeywordTok{SolveEigenProblem}\NormalTok{(withinScatterMatrix, betweenScatterMatrix)}
  
  \CommentTok{#transform the samples onto the new subspace}
\NormalTok{  y <-}\StringTok{ }\NormalTok{(}\KeywordTok{as.matrix}\NormalTok{(mydata) }\OperatorTok{%*%}\StringTok{ }\NormalTok{ei}\OperatorTok{$}\NormalTok{vectors[,}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{])}
  
  \CommentTok{## visual comparison with PCA}
  \KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\NormalTok{  pca <-}\StringTok{ }\KeywordTok{prcomp}\NormalTok{(mydata)}
  \KeywordTok{plot}\NormalTok{(y[,}\DecValTok{1}\NormalTok{], y[,}\DecValTok{2}\NormalTok{], }\DataTypeTok{col =}\NormalTok{ labels, }\DataTypeTok{pch =} \DecValTok{21}\NormalTok{, }\DataTypeTok{lwd =} \DecValTok{2}\NormalTok{, }\DataTypeTok{xlab =} \StringTok{"LD1"}\NormalTok{ , }\DataTypeTok{ylab =} \StringTok{"LD2"}\NormalTok{, }\DataTypeTok{main =} \StringTok{"LDA"}\NormalTok{)}
  \KeywordTok{plot}\NormalTok{(}\OperatorTok{-}\NormalTok{pca}\OperatorTok{$}\NormalTok{x, }\DataTypeTok{col =}\NormalTok{ labels, }\DataTypeTok{pch =} \DecValTok{21}\NormalTok{, }\DataTypeTok{lwd =} \DecValTok{2}\NormalTok{, }\DataTypeTok{main =} \StringTok{"PCA"}\NormalTok{)}

  \KeywordTok{return}\NormalTok{(ei}\OperatorTok{$}\NormalTok{vectors)}
\NormalTok{\}}

\CommentTok{############################# FUNCTIONS }\RegionMarkerTok{END}\CommentTok{ ###################################}


\CommentTok{############################# MAIN ##########################################}

\CommentTok{### PREPARE DATA}
\CommentTok{#data(iris)}
\CommentTok{#mydata <- iris}
\CommentTok{#labels <- mydata[,5]}
\CommentTok{#mydata <- mydata[,-5]}

\NormalTok{mydata <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"wine.csv"}\NormalTok{, }\DataTypeTok{header =} \OtherTok{FALSE}\NormalTok{)}
\NormalTok{labels <-}\StringTok{ }\NormalTok{mydata[,}\DecValTok{1}\NormalTok{]}
\NormalTok{labels <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(labels)}
\NormalTok{mydata <-}\StringTok{ }\NormalTok{mydata[,}\OperatorTok{-}\DecValTok{1}\NormalTok{]}

\CommentTok{#compute LDA and return corresponding eigenvectors}
\NormalTok{eigenLDA <-}\StringTok{ }\KeywordTok{LDA}\NormalTok{(mydata, labels)}
\CommentTok{#find centroids in the transformed data}
\NormalTok{centroids <-}\StringTok{ }\KeywordTok{ComputeCentroids}\NormalTok{(}\KeywordTok{as.matrix}\NormalTok{(mydata) }\OperatorTok{%*%}\StringTok{ }\NormalTok{eigenLDA[,}\DecValTok{1}\OperatorTok{:}\NormalTok{(}\KeywordTok{length}\NormalTok{(}\KeywordTok{levels}\NormalTok{(labels))}\OperatorTok{-}\DecValTok{1}\NormalTok{)],}
                              \DataTypeTok{labels =}\NormalTok{ labels)}
\CommentTok{#make predictions on the "mydata"}
\NormalTok{prediction <-}\StringTok{ }\KeywordTok{Classify}\NormalTok{(}\DataTypeTok{newData =}\NormalTok{ mydata, }\DataTypeTok{labels =}\NormalTok{ labels, }\DataTypeTok{eigenVectors =}\NormalTok{ eigenLDA,}
         \DataTypeTok{centroids =}\NormalTok{ centroids)}
\CommentTok{#ACC}
\KeywordTok{sum}\NormalTok{(prediction }\OperatorTok{==}\StringTok{ }\NormalTok{labels)}\OperatorTok{/}\NormalTok{(}\KeywordTok{length}\NormalTok{(labels))}

\CommentTok{#CrossValidation}
\NormalTok{accLDA <-}\StringTok{ }\KeywordTok{CrossvalidationLDA}\NormalTok{(mydata, labels, }\DataTypeTok{kfolds =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{discuss-given-results.}{%
\subparagraph{Discuss given results.}\label{discuss-given-results.}}


\end{document}
